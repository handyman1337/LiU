{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add your LiU-ID here:**\n",
    "* <liuid 1>\n",
    "* <liuid 2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0. Quick introduction to jupyter notebooks**\n",
    "* Each cell in this notebook contains either code or text.\n",
    "* You can run a cell by pressing Ctrl-Enter, or run and advance to the next cell with Shift-Enter.\n",
    "* Code cells will print their output, including images, below the cell. Running it again deletes the previous output, so be careful if you want to save some reuslts.\n",
    "* You don't have to rerun all cells to test changes, just rerun the cell you have made changes to. Some exceptions might apply, for example if you overwrite variables from previous cells, but in general this will work.\n",
    "* If all else fails, use the \"Kernel\" menu and select \"Restart Kernel and Clear All Output\". You can also use this menu to run all cells.\n",
    "\n",
    "### **0.5 Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #ffb8b8ff; padding: 5px; border-radius: 5px; font-weight: bold; color: #800000ff; font-size: 1.2em;\">\n",
    "This notebook is entirely optional, and only meant for those who want to dive deeper.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules when changed\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# Plot figures \"inline\" with other output\n",
    "%matplotlib inline\n",
    "\n",
    "# Import modules, classes, functions\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import loadDataset, splitData, plotTrainingProgress, plotResultsDots, plotConfusionMatrixOCR\n",
    "from evalFunctions import calcAccuracy, calcConfusionMatrix\n",
    "\n",
    "# Configure nice figures\n",
    "plt.rcParams['figure.facecolor']='white'\n",
    "plt.rcParams['figure.figsize']=(8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. N-layer neural network**\n",
    "\n",
    "You will now implement a more general N-layer neural network, where you can freely choose how many layers to use for each model. This means that the inputs and outputs from the functions, such as the weights and biases, are passed as lists. The structure of the code is otherwise still the same, with a `forward`, `backward`, and `update` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1 Implementing the forward pass**\n",
    "\n",
    "This implementation will look similar to the previous tasks, except most inputs and outputs are now lists of matrices. Additionally, the previous input parameter \"useTanhOutput\" is removed, and instead the activation function for each layer is passed as a list of strings. You can implement support for different activation functions, such as \"tanh\", \"relu\", \"lrelu\" (leaky ReLU), and \"linear\". Note that the activation function for the output layer is also specified in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W, B, activations):\n",
    "    \"\"\"Forward pass of two layer network\n",
    "\n",
    "    Args:\n",
    "        X (array): Input samples.\n",
    "        W (list of arrays): Neural network weights for each layer.\n",
    "        B (list of arrays): Neural network biases for each layer.\n",
    "        activations (list of str): Activation functions for each layer, including output layer.\n",
    "            You can support whichever you want, recommended 'linear', 'tanh', and 'relu'.\n",
    "\n",
    "    Returns:\n",
    "        Y (array): Output for each sample and class.\n",
    "        L (array): Resulting label of each sample.\n",
    "        U (list of arrays): Output of each hidden layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    # Initialize list to store outputs of each layer\n",
    "    U = [None] * len(W)\n",
    "\n",
    "    # Initialize a working variable to hold the current value of\n",
    "    # Y as we propagate through the network\n",
    "    YTmp = ???\n",
    "\n",
    "    # Loop through hidden layers\n",
    "    for i = ???\n",
    "        # Calculate layer output based on previous layer output\n",
    "        YTmp = ???\n",
    "\n",
    "        # Apply activation function for current layer\n",
    "        if activations[i] == 'linear':\n",
    "            YTmp = ???\n",
    "        elif activations[i] == 'tanh':\n",
    "            YTmp = ???\n",
    "        # Etc...\n",
    "        \n",
    "        # Store output of current layer, including output layer\n",
    "        U[i] = YTmp\n",
    "\n",
    "    # ============================================\n",
    "    \n",
    "    Y = YTmp\n",
    "\n",
    "    # Calculate labels\n",
    "    L = Y.argmax(axis=1)\n",
    "\n",
    "    return Y, L, U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2 Implementing the backward pass**\n",
    "\n",
    "To implement the backward pass, you will again need to loop over all layers, this time in reverse order. Since we do not know exlicitly the number of layers, you will have to implement the gradient calculation iteratively instead of unrolling it for each layer, like we did in the two-layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(W, B, X, U, Y, D, activations):\n",
    "    \"\"\"Compute the gradients for network weights and biases\n",
    "\n",
    "    Args:\n",
    "        W (list of arrays): Current values of the network weights for each layer.\n",
    "        B (list of arrays): Current values of the network biases for each layer.\n",
    "        X (array): Training samples.\n",
    "        U (list of arrays): Intermediate outputs of the hidden layers.\n",
    "        Y (array): Predicted outputs.\n",
    "        D (array): Target outputs.\n",
    "        activations (list of str): Activation functions for each layer, including output layer.\n",
    "            You can support whichever you want, recommended 'linear', 'tanh', and 'relu'.\n",
    "\n",
    "    Returns:\n",
    "        GradW (list of arrays): Gradients with respect to the network weights for each layer.\n",
    "        GradB (list of arrays): Gradients with respect to the network biases for each layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    N  = Y.shape[0]\n",
    "    NC = Y.shape[1]\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "\n",
    "    # Initialize lists to store gradients\n",
    "    GradW = [None] * len(W)\n",
    "    GradB = [None] * len(B)\n",
    "\n",
    "    # Compute initial delta (error) with scaling\n",
    "    delta = ???\n",
    "\n",
    "    # Loop backwards through layers\n",
    "    for i in reversed(range(len(W))):\n",
    "        # Get input to current layer\n",
    "        In = ???\n",
    "\n",
    "        # Apply derivative of activation function\n",
    "        if activations[i] == \"tanh\":\n",
    "            delta *= ???\n",
    "        elif activations[i] == \"relu\":\n",
    "            delta *= ???\n",
    "        # Etc...\n",
    "\n",
    "        # Compute gradients for weights and biases\n",
    "        GradW[i] = ???\n",
    "        GradB[i] = ???\n",
    "\n",
    "        # Update delta for next layer (if not input layer)\n",
    "        if i > 0:\n",
    "            delta = ???\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    return GradW, GradB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3 Implementing the weight update**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(W, B, GradW, GradB, params):\n",
    "    \"\"\"Update weights and biases using computed gradients.\n",
    "\n",
    "    Args:\n",
    "        W (list of arrays): Current values of the network weights for each layer.\n",
    "        B (list of arrays): Current values of the network biases for each layer.\n",
    "        \n",
    "        GradW (list of arrays): Gradients with respect to the network weights for each layer.\n",
    "        GradB (list of arrays): Gradients with respect to the network biases for each layer.\n",
    "        \n",
    "        params (dict):\n",
    "            - learningRate: Scale factor for update step.\n",
    "            - momentum: Scale factor for momentum update (optional).\n",
    "        \n",
    "    Returns:\n",
    "        W (list of arrays): Updated network weights for each layer.\n",
    "        B (list of arrays): Updated network biases for each layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    LR = params[\"learningRate\"]\n",
    "    \n",
    "    # For optional task on momentum\n",
    "    M = params[\"momentum\"]\n",
    "    PrevGradW = params[\"PrevGradW\"]\n",
    "    PrevGradB = params[\"PrevGradB\"]\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    # Tip, use list comprehensions to compute the updates in a concise way\n",
    "    \n",
    "    TotGradW = ???\n",
    "    TotGradB = ???\n",
    "    \n",
    "    W = ???\n",
    "    B = ???\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    # For optinal task on momentum\n",
    "    params[\"PrevGradW\"] = TotGradW\n",
    "    params[\"PrevGradB\"] = TotGradB\n",
    "    \n",
    "    return W, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.4 The training function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMultiLayer(XTrain, DTrain, XTest, DTest, W0, B0, activations, params):\n",
    "    \"\"\"Trains a N-layer network\n",
    "\n",
    "    Args:\n",
    "        XTrain (array): Training samples.\n",
    "        DTrain (array): Training network target values.\n",
    "        XTest (array): Test samples.\n",
    "        DTest (array): Test network target values.\n",
    "        W0 (list of arrays): Initial values of the network weights for each layer.\n",
    "        B0 (list of arrays): Initial values of the network biases for each layer.\n",
    "        activations (list of str): Activation functions for each layer, including output layer.\n",
    "        params (dict): Dictionary containing:\n",
    "            epochs (int): Number of training steps.\n",
    "            learningRate (float): Size of a training step.\n",
    "\n",
    "    Returns:\n",
    "        W (list of arrays): Network weights after training.\n",
    "        B (list of arrays): Network biases after training.\n",
    "        metrics (dict): Losses and accuracies for training and test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables\n",
    "    metrics = {keys:np.zeros(params[\"epochs\"]+1) for keys in [\"lossTrain\", \"lossTest\", \"accTrain\", \"accTest\"]}\n",
    "\n",
    "    # Set default activations if none provided\n",
    "    if activations is None:\n",
    "        activations = [\"tanh\"] * (len(W0) - 1) + [\"linear\"]\n",
    "\n",
    "    if \"momentum\" not in params:\n",
    "        params[\"momentum\"] = 0\n",
    "\n",
    "    nTrain = XTrain.shape[0]\n",
    "    nTest  = XTest.shape[0]\n",
    "    nClasses = DTrain.shape[1]\n",
    "    \n",
    "    # For optinal task on momentum\n",
    "    params[\"PrevGradW\"] = [np.zeros_like(W) for W in W0]\n",
    "    params[\"PrevGradB\"] = [np.zeros_like(B) for B in B0]\n",
    "    \n",
    "    # Set initial weights and biases\n",
    "    W = W0\n",
    "    B = B0\n",
    "\n",
    "    # Get class labels\n",
    "    LTrain = np.argmax(DTrain, axis=1)\n",
    "    LTest  = np.argmax(DTest , axis=1)\n",
    "\n",
    "    # Calculate initial metrics\n",
    "    YTrain, LTrainPred, UTrain = forward(XTrain, W, B, activations)\n",
    "    YTest , LTestPred , _      = forward(XTest , W, B, activations)\n",
    "    \n",
    "    # Including the initial metrics makes the progress plots worse, set nan to exclude\n",
    "    metrics[\"lossTrain\"][0] = np.nan # ((YTrain - DTrain)**2).mean()\n",
    "    metrics[\"lossTest\"][0]  = np.nan # ((YTest  - DTest )**2).mean()\n",
    "    metrics[\"accTrain\"][0]  = np.nan # (LTrainPred == LTrain).mean()\n",
    "    metrics[\"accTest\"][0]   = np.nan # (LTestPred  == LTest ).mean()\n",
    "\n",
    "    # Create figure for plotting progress\n",
    "    fig = plt.figure(figsize=(20,8), tight_layout=True)\n",
    "\n",
    "    # Training loop\n",
    "    for n in range(1, params[\"epochs\"]+1):\n",
    "        \n",
    "        # Compute gradients...\n",
    "        GradW, GradB = backward(W, B, XTrain, UTrain, YTrain, DTrain, activations)\n",
    "        # ... and update weights\n",
    "        W, B = update(W, B, GradW, GradB, params)\n",
    "        \n",
    "        # Evaluate errors\n",
    "        YTrain, LTrainPred, UTrain = forward(XTrain, W, B, activations)\n",
    "        YTest , LTestPred , _      = forward(XTest , W, B, activations)\n",
    "        metrics[\"lossTrain\"][n] = ((YTrain - DTrain)**2).mean()\n",
    "        metrics[\"lossTest\"][n]  = ((YTest  - DTest )**2).mean()\n",
    "        metrics[\"accTrain\"][n]  = (LTrainPred == LTrain).mean()\n",
    "        metrics[\"accTest\"][n]   = (LTestPred  == LTest ).mean()\n",
    "\n",
    "        # Plot progress\n",
    "        if (not n % (params[\"epochs\"] // 25)) or (n == params[\"epochs\"]):\n",
    "            plotMode = \"network\" if W[0].shape[0] < 64 else \"ocr\"\n",
    "            plotTrainingProgress(fig, W, B, metrics, n=n, cmap='coolwarm', mode=plotMode)\n",
    "\n",
    "    return W, B, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the same function for normalizing the data, which is even more important now that we have more than one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X, XRef):\n",
    "    \"\"\"\n",
    "    Normalizes the data X with the mean and standard deviation of the reference data XRef. These can be the same dataset.\n",
    "    \n",
    "    Args:\n",
    "       X (array): Data matrix to be normalized, features are in axis 0.\n",
    "       XRef (array): Data matrix for calculating the normalization parameters, features are in axis 0.\n",
    "       \n",
    "    Returns:\n",
    "       X (array): Input X normalized with XRef parameters.\n",
    "    \"\"\"\n",
    "    # Compute mean and std of the reference data set\n",
    "    m = XRef.mean(axis=0)\n",
    "    s = XRef.std(axis=0)\n",
    "    # Prevent division by 0 is feature has no variance\n",
    "    s[s == 0] = 1\n",
    "    # Return normalized data\n",
    "    return (X - m) / s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **2 Optimizing each dataset**\n",
    "\n",
    "Like before, we define a function that performs the steps for training the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  trainMultiLayerOnDataset(datasetNr, testSplit, W0, B0, activations, params):\n",
    "    \"\"\"Train a multi layer network on a specific dataset.\n",
    "\n",
    "    Ags:\n",
    "        datasetNr (int): ID of dataset to use\n",
    "        testSplit (float): Fraction of data reserved for testing.\n",
    "        W0 (list of arrays): Initial values of the network weights for each layer.\n",
    "        B0 (list of arrays): Initial values of the network biases for each layer.\n",
    "        activations (list of str): Activation functions for each layer, including output layer.\n",
    "        params (dict): Dictionary containing:\n",
    "            epochs (int): Number of training steps.\n",
    "            learningRate (float): Size of a training step.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data and split into training and test sets\n",
    "    X, D, L = loadDataset(datasetNr)\n",
    "    XTrain, DTrain, LTrain, XTest, DTest, LTest = splitData(X, D, L, testSplit)\n",
    "    \n",
    "    if \"normalize\" in params and params[\"normalize\"]:\n",
    "        XTrainNorm = normalize(XTrain, XTrain)\n",
    "        XTestNorm  = normalize(XTest, XTrain)\n",
    "    else:\n",
    "        XTrainNorm = XTrain\n",
    "        XTestNorm  = XTest\n",
    "\n",
    "    # Train network\n",
    "    W, B, metrics = trainMultiLayer(XTrainNorm, DTrain, XTestNorm, DTest, W0, B0, activations, params)\n",
    "\n",
    "    # Predict classes on test set\n",
    "    LPredTrain = forward(XTrainNorm, W, B, activations)[1]\n",
    "    LPredTest  = forward(XTestNorm , W, B, activations)[1]\n",
    "\n",
    "    # Compute metrics\n",
    "    accTrain = calcAccuracy(LPredTrain, LTrain)\n",
    "    accTest  = calcAccuracy(LPredTest , LTest)\n",
    "    confMatrix = calcConfusionMatrix(LPredTest, LTest)\n",
    "\n",
    "    # Display results\n",
    "    print(f'Train accuracy: {accTrain:.4f}')\n",
    "    print(f'Test accuracy: {accTest:.4f}')\n",
    "    print(\"Test data confusion matrix:\")\n",
    "    print(confMatrix)\n",
    "\n",
    "    if datasetNr < 4:\n",
    "        plotResultsDots(XTrainNorm, LTrain, LPredTrain, XTestNorm, LTest, LPredTest, lambda X: forward(X, W, B, activations)[1])\n",
    "    else:\n",
    "        plotConfusionMatrixOCR(XTest, LTest, LPredTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1 Optimizing dataset 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "nInputs = ???\n",
    "nClasses = ???\n",
    "nHidden = [???, ???]\n",
    "\n",
    "# Weights are expected to be lists of np.arrays\n",
    "W0 = [???, ???]\n",
    "B0 = [???, ???]\n",
    "      \n",
    "# Activations are expected to be a list of strings with activation function names, including output activation\n",
    "activations = [\"???\", \"???\", \"???\"]\n",
    "\n",
    "params = {\"epochs\": 2000, \"learningRate\": 0.05, \"normalize\": True, \"momentum\": 0.9}\n",
    "\n",
    "# ============================================\n",
    "\n",
    "trainMultiLayerOnDataset(1, 0.15, W0, B0, activations, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2 Optimizing dataset 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "nInputs = ???\n",
    "nClasses = ???\n",
    "nHidden = [???, ???]\n",
    "\n",
    "# Weights are expected to be lists of np.arrays\n",
    "W0 = [???, ???]\n",
    "B0 = [???, ???]\n",
    "      \n",
    "# Activations are expected to be a list of strings with activation function names, including output activation\n",
    "activations = [\"???\", \"???\", \"???\"]\n",
    "\n",
    "params = {\"epochs\": 2000, \"learningRate\": 0.05, \"normalize\": True, \"momentum\": 0.9}\n",
    "\n",
    "# ============================================\n",
    "\n",
    "trainMultiLayerOnDataset(2, 0.15, W0, B0, activations, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3 Optimizing dataset 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "nInputs = ???\n",
    "nClasses = ???\n",
    "nHidden = [???, ???]\n",
    "\n",
    "# Weights are expected to be lists of np.arrays\n",
    "W0 = [???, ???]\n",
    "B0 = [???, ???]\n",
    "      \n",
    "# Activations are expected to be a list of strings with activation function names, including output activation\n",
    "activations = [\"???\", \"???\", \"???\"]\n",
    "\n",
    "params = {\"epochs\": 2000, \"learningRate\": 0.05, \"normalize\": True, \"momentum\": 0.9}\n",
    "\n",
    "# ============================================\n",
    "\n",
    "trainMultiLayerOnDataset(3, 0.15, W0, B0, activations, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.4 Optimizing dataset 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "nInputs = ???\n",
    "nClasses = ???\n",
    "nHidden = [???, ???]\n",
    "\n",
    "# Weights are expected to be lists of np.arrays\n",
    "W0 = [???, ???]\n",
    "B0 = [???, ???]\n",
    "      \n",
    "# Activations are expected to be a list of strings with activation function names, including output activation\n",
    "activations = [\"???\", \"???\", \"???\"]\n",
    "\n",
    "params = {\"epochs\": 2000, \"learningRate\": 0.05, \"normalize\": True, \"momentum\": 0.9}\n",
    "\n",
    "# ============================================\n",
    "\n",
    "trainMultiLayerOnDataset(4, 0.15, W0, B0, activations, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TBMI26_New",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
